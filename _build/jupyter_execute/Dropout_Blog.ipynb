{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f8d8d6-5279-47f1-a651-ec36731fc993",
   "metadata": {},
   "source": [
    "# Drop It to Improve It: How Dropout Prevents Overfitting\n",
    "by Honey Angel Pabololot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef67db-1c92-460a-8ad6-8702f9d88ddb",
   "metadata": {},
   "source": [
    "### üéØ Introduction\n",
    "\n",
    "Deep learning models are powerful, but they often suffer from overfitting‚Äîperforming well on training data but poorly on new data.\n",
    "A widely used solution is Dropout, a simple yet effective technique where random neurons are turned off during training to force the network to generalize better.\n",
    "\n",
    "This blog explores how different dropout rates affect training behavior, using actual experiment results from a CNN trained on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9439836-f1bb-44c6-a7a2-621ee099bd96",
   "metadata": {},
   "source": [
    "### üîç What Is Dropout?\n",
    "\n",
    "Dropout was introduced by Srivastava et al. (2014) as a regularization technique for neural networks. Dropout randomly disables a fraction p of neurons during training.\n",
    "This prevents the model from depending too heavily on small sets of neurons. During training, every neuron has a probability  ùëù of being ‚Äúdropped‚Äù (set to zero).\n",
    "\n",
    "This creates many different ‚Äúthinned networks‚Äù during training and prevents co-adaptation of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44200a-5dd9-42d4-91d7-77dc2ff322ad",
   "metadata": {},
   "source": [
    "### Why it Works\n",
    "\n",
    "‚úî Prevents co-adaptation\n",
    "\n",
    "Neurons cannot rely on specific other neurons. They learn redundant, generalizable features.\n",
    "\n",
    "‚úî Acts like training multiple models\n",
    "\n",
    "Each dropout configuration is like a new sub-model. The final model behaves like an ensemble, improving generalization.\n",
    "\n",
    "‚úî Reduces overfitting\n",
    "\n",
    "When dropout is used correctly, test accuracy increases while training accuracy decreases slightly (which is good)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8707f-7066-4165-9fbd-9433f8d34011",
   "metadata": {},
   "source": [
    "Dropout improves:\n",
    "\n",
    "- Generalization\n",
    "\n",
    "- Model robustness\n",
    "\n",
    "- Ensemble-like behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce389c97-e245-46c1-a88f-429d84e5c013",
   "metadata": {},
   "source": [
    "### üß™ Experiment Setup \n",
    "\n",
    "**Dataset**: MNIST\n",
    "\n",
    "**Model**: Convolutional Neural Network (CNN)\n",
    "\n",
    "**Dropout Values Tested**: 0.0, 0.2, 0.5, 0.7\n",
    "\n",
    "**Epochs**: 5\n",
    "\n",
    "**Metrics**: Training loss, validation loss, accuracy, runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd92479-b64c-4804-bf35-a7634f69e1b1",
   "metadata": {},
   "source": [
    "### üìä Results\n",
    "\n",
    "##### üîΩ Dropout = 0.0\n",
    "\n",
    "Training Loss: 0.0150\n",
    "\n",
    "Validation Loss: 0.0562\n",
    "\n",
    "##### Test Accuracy\n",
    "\n",
    "‚úî 98.93%\n",
    "\n",
    "##### Runtime\n",
    "\n",
    "‚è± 629.55 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1e533-1b43-4ad9-8182-31efedd78462",
   "metadata": {},
   "source": [
    "#### üîΩ Dropout = 0.2\n",
    "\n",
    "Training Loss: 0.0206\n",
    "\n",
    "Validation Loss: 0.0420\n",
    "\n",
    "##### Test Accuracy\n",
    "\n",
    "‚úî 99.03%\n",
    "\n",
    "##### Runtime\n",
    "\n",
    "‚è± 661.27 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9ea0f-c16b-4fc4-bb88-521916a4ed65",
   "metadata": {},
   "source": [
    "#### üîΩ Dropout = 0.5\n",
    "\n",
    "Training Loss: 0.0341\n",
    "\n",
    "Validation Loss: 0.0420\n",
    "\n",
    "##### Test Accuracy\n",
    "\n",
    "‚úî 99.02%\n",
    "\n",
    "##### Runtime\n",
    "\n",
    "‚è± 645.39 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed40da-85fd-4e01-9a70-43a1191288f4",
   "metadata": {},
   "source": [
    "#### üîΩ Dropout = 0.7\n",
    "\n",
    "Training Loss: 0.0529\n",
    "\n",
    "Validation Loss: 0.0430\n",
    "\n",
    "##### Test Accuracy\n",
    "\n",
    "‚úî 99.06%\n",
    "\n",
    "##### Runtime\n",
    "\n",
    "‚è± 684.78 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54545ea2-95ec-474d-be9c-a5df6bf4e671",
   "metadata": {},
   "source": [
    "### üß† Analysis of Findings\n",
    "\n",
    "1. Observations When Changing Dropout\n",
    "\n",
    "- Low dropout (0.0‚Äì0.2):\n",
    "\n",
    "    - The model learned very quickly, as seen in the rapidly decreasing training loss.\n",
    "\n",
    "    - Validation loss initially decreased but then showed some fluctuations, indicating a risk of overfitting.\n",
    "\n",
    "- Moderate dropout (0.5):\n",
    "\n",
    "    - Training loss decreased more slowly, but validation loss remained stable across epochs.\n",
    "\n",
    "    - Test accuracy remained high (99.02%), showing a good balance between learning and generalization.\n",
    "\n",
    "- High dropout (0.7):\n",
    "\n",
    "    - Training loss decreased slower compared to lower dropout rates.\n",
    "\n",
    "    - Validation loss stayed consistent, indicating stable generalization, and test accuracy was slightly higher (99.06%).\n",
    "\n",
    "    - Runtime increased slightly due to more neurons being dropped per batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab233e-a4aa-4b7d-990d-9f539199c330",
   "metadata": {},
   "source": [
    "2. Effect on Performance and Stability\n",
    "\n",
    "- Performance:\n",
    "\n",
    "    All dropout rates produced very high test accuracy (98.93%‚Äì99.06%), showing that the network is robust even with higher dropout.\n",
    "\n",
    "- Stability:\n",
    "\n",
    "    Moderate to high dropout rates (0.2‚Äì0.7) improved stability of validation loss, reducing fluctuations and overfitting compared to no dropout (0.0).\n",
    "\n",
    "Dropout helped the network generalize better, confirming its role as a regularization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34c351-99a6-44fb-805d-b7d90349be72",
   "metadata": {},
   "source": [
    "3. Insights for Future Model Tuning\n",
    "\n",
    "- Start with moderate dropout (0.2‚Äì0.5) for dense layers ‚Äî it balances generalization and training speed.\n",
    "\n",
    "- High dropout (0.7) can be used if overfitting is severe, but expect slower convergence.\n",
    "\n",
    "- No dropout (0.0) can be risky for longer training or smaller datasets, as overfitting may occur.\n",
    "\n",
    "- Always monitor validation loss and test accuracy, not just training loss, to choose the optimal dropout rate.\n",
    "\n",
    "- Dropout can be combined with other regularization techniques for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cad160-f4d7-4e4e-8d97-7f096ecab389",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "Dropout is simple yet highly effective. By selectively ‚Äúforgetting‚Äù neurons during training, the network becomes more robust and generalizes better. Your experiment confirms that moderate dropout is usually the sweet spot, while very high dropout still works but slows training.\n",
    "\n",
    "This confirms the core idea behind dropout:\n",
    "\n",
    "**\"Neural networks learn better when they forget a little.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3df428-bd5e-4c86-b741-ba7cdc5ff7ba",
   "metadata": {},
   "source": [
    "For detailed experimentation, see [Dropout Experiment](dropout_experiment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedfdfa-080f-4388-99d0-640aee553b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}